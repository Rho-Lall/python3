{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble: A Brand new Jay\n",
    "\n",
    "After an eventful season on season 8 of *A Brand New Jay*, the 3 remaining contestants were invited to Jay Stacksby's private island for the last three episodes. When the day of filming the finale came Mr. Stacksby was found with one of his Professional Series 8-inch Chef Knives plunged through his heart! After the initial investigation highlighted that the film crew all lived in a separate house on the other side of the island, it was concluded that only the three contestants were near enough to Stacksby in order to commit a crime. At the scene of the crime, a letter was left. Here are the contents of that letter:\n",
    "\n",
    "> You may call me heartless, a killer, a monster, a murderer, but I'm still NOTHING compared to the villian that Jay was. This whole contest was a sham, an elaborate plot to shame the contestants and feed Jay's massive, massive ego. SURE you think you know him! You've seen him smiling for the cameras, laughing, joking, telling stories, waving his money around like a prop but off camera he was a sinister beast, a cruel cruel taskmaster, he treated all of us like slaves, like cattle, like animals! Do you remember Lindsay, she was the first to go, he called her such horrible things that she cried all night, keeping up all up, crying, crying, and more crying, he broke her with his words. I miss my former cast members, all of them very much. And we had to live with him, live in his home, live in his power, deal with his crazy demands. AND FOR WHAT! DID YOU KNOW THAT THE PRIZE ISN'T REAL? He never intended to marry one of us! The carrot on the stick was gone, all that was left was stick, he told us last night that we were all a terrible terrible disappointment and none of us would ever amount to anything, and that regardless of who won the contest he would never speak to any of us again! It's definitely the things like this you can feel in your gut how wrong he is! Well I showed him, he got what he deserved all right, I showed him, I showed him the person I am! I wasn't going to be pushed around any longer, and I wasn't going to let him go on pretending that he was some saint when all he was was a sick sick twisted man who deserved every bit of what he got. The fans need to know, Jay Stacksby is a vile amalgamation of all things evil and bad and the world is a better place without him.\n",
    "\n",
    "Pretty sinister stuff! Luckily, in addition to this bold-faced admission, we have the introduction letters of the three contestants. Maybe there is a way to use this information to determine who the author of this murder letter is?\n",
    "\n",
    "Myrtle Beech's introduction letter:\n",
    "> Salutations. My name? Myrtle. Myrtle Beech. I am a woman of simple tastes. I enjoy reading, thinking, and doing my taxes. I entered this competition because I want a serious relationship. I want a commitment. The last man I dated was too whimsical. He wanted to go on dates that had no plan. No end goal. Sometimes we would just end up wandering the streets after dinner. He called it a \"walk\". A \"walk\" with no destination. Can you imagine? I like every action I take to have a measurable effect. When I see a movie, I like to walk away with insights that I did not have before. When I take a bike ride, there better be a worthy destination at the end of the bike path. Jay seems frivolous at times. This worries me. However, it is my staunch belief that one does not make and keep money without having a modicum of discipline. As such, I am hopeful. I will now list three things I cannot live without. Water. Emery boards. Dogs. Thank you for the opportunity to introduce myself. I look forward to the competition. \n",
    "\n",
    "Lily Trebuchet's introduction letter:\n",
    "> Hi, I'm Lily Trebuchet from East Egg, Long Island. I love cats, hiking, and curling up under a warm blanket with a book. So they gave this little questionnaire to use for our bios so lets get started. What are some of my least favorite household chores? Dishes, oh yes it's definitely the dishes, I just hate doing them, don't you? Who is your favorite actor and why? Hmm, that's a hard one, but I think recently I'll have to go with Michael B. Jordan, every bit of that man is handsome, HANDSOME! Do you remember seeing him shirtless? I can't believe what he does for the cameras! Okay okay next question, what is your perfect date? Well it starts with a nice dinner at a delicious but small restaurant, you know like one of those places where the owner is in the back and comes out to talk to you and ask you how your meal was. My favorite form of art? Another hard one, but I think I'll have to go with music, music you can feel in your whole body and it is electrifying and best of all, you can dance to it! Okay final question, let's see, What are three things you cannot live without? Well first off, my beautiful, beautiful cat Jerry, he is my heart and spirit animal. Second is pasta, definitely pasta, and the third I think is my family, I love all of them very much and they support me in everything I do. I know Jay Stacksby is a handsome man and all of us want to be the first to walk down the aisle with him, but I think he might truly be the one for me. Okay that's it for the bio, I hope you have fun watching the show! \n",
    "\n",
    "Gregg T Fishy's introduction letter:\n",
    "\n",
    "> A most good day to you all, I am Gregg T Fishy, of the Fishy Enterprise fortune. I am 37 years young, an adventurous spirit and I've never lost my sense of childlike wonder. I do love to be in the backyard gardening and I have the most extraordinary time when I'm fishing. Fishing for what, you might find yourself asking? Why, I happen to always be fishing for compliments of course! I have a stunning pair of radiant blue eyes that will pierce the soul of anyone who dare gaze upon my countenance. I quite enjoy going on long jaunts through garden paths and short walks through greenhouses. I hope that Jay will be as absolutely interesting as he appears on the television, I find that he has some of the most curious tastes in style and humor. When I'm out and about I quite enjoy hearing tales that instill in my heart of hearts the fascination that beguiles my every day life, every fiber of my being scintillates and vascillates with extreme pleasure during one of these charming anecdotes and significantly pleases my beautiful personage. I cannot wait to enjoy being on the television program A Jay To Remember, it certainly seems like a grand time to explore life and love."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who Do I Think Dunnit? (My initial guess.)\n",
    "\n",
    "At the onset of this excercise I am taking a guess as to who I think the killer is. I don't think it is Greg. He uses a lot of big words and referes to himself alot (my, I). The murder didn't do this. Myrtle Beech has a similar self focus. She writes about ideas and things. Lily, uses the word \"you\" more than the other two. She also is the only contestant who ALLCAPS select words. Of the three she is the most emotionally invested of the bunch. \n",
    "        \n",
    "I wonder if I have been dupped by one of the other two, who used the ALLCAPS to frame Lily. I don't think so. I think it is her: emotion, use of the word \"you\", and the focus on relationships with others. AND OFCOUSE THE USE OF ALLCAPS. \n",
    "         \n",
    "Is it Lily Trebuchet? We shall see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving The Different Examples as Variables\n",
    "\n",
    "First let's create variables to hold the text data in! Save the muder note as a string in a variable called `murder_note`. Save Lily Trebuchet's introduction into `lily_trebuchet_intro`. Save Myrtle Beech's introduction into `myrtle_beech_intro`. Save Gregg T Fishy's introduction into `gregg_t_fishy_intro`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "murder_note = \"You may call me heartless, a killer, a monster, a murderer, but I'm still NOTHING compared to the villian that Jay was. This whole contest was a sham, an elaborate plot to shame the contestants and feed Jay's massive, massive ego. SURE you think you know him! You've seen him smiling for the cameras, laughing, joking, telling stories, waving his money around like a prop but off camera he was a sinister beast, a cruel cruel taskmaster, he treated all of us like slaves, like cattle, like animals! Do you remember Lindsay, she was the first to go, he called her such horrible things that she cried all night, keeping up all up, crying, crying, and more crying, he broke her with his words. I miss my former cast members, all of them very much. And we had to live with him, live in his home, live in his power, deal with his crazy demands. AND FOR WHAT! DID YOU KNOW THAT THE PRIZE ISN'T REAL? He never intended to marry one of us! The carrot on the stick was gone, all that was left was stick, he told us last night that we were all a terrible terrible disappointment and none of us would ever amount to anything, and that regardless of who won the contest he would never speak to any of us again! It's definitely the things like this you can feel in your gut how wrong he is! Well I showed him, he got what he deserved all right, I showed him, I showed him the person I am! I wasn't going to be pushed around any longer, and I wasn't going to let him go on pretending that he was some saint when all he was was a sick sick twisted man who deserved every bit of what he got. The fans need to know, Jay Stacksby is a vile amalgamation of all things evil and bad and the world is a better place without him.\"\n",
    "lily_trebuchet_intro = \"Hi, I'm Lily Trebuchet from East Egg, Long Island. I love cats, hiking, and curling up under a warm blanket with a book. So they gave this little questionnaire to use for our bios so lets get started. What are some of my least favorite household chores? Dishes, oh yes it's definitely the dishes, I just hate doing them, don't you? Who is your favorite actor and why? Hmm, that's a hard one, but I think recently I'll have to go with Michael B. Jordan, every bit of that man is handsome, HANDSOME! Do you remember seeing him shirtless? I can't believe what he does for the cameras! Okay okay next question, what is your perfect date? Well it starts with a nice dinner at a delicious but small restaurant, you know like one of those places where the owner is in the back and comes out to talk to you and ask you how your meal was. My favorite form of art? Another hard one, but I think I'll have to go with music, music you can feel in your whole body and it is electrifying and best of all, you can dance to it! Okay final question, let's see, What are three things you cannot live without? Well first off, my beautiful, beautiful cat Jerry, he is my heart and spirit animal. Second is pasta, definitely pasta, and the third I think is my family, I love all of them very much and they support me in everything I do. I know Jay Stacksby is a handsome man and all of us want to be the first to walk down the aisle with him, but I think he might truly be the one for me. Okay that's it for the bio, I hope you have fun watching the show!\"\n",
    "myrtle_beech_intro = \"Salutations. My name? Myrtle. Myrtle Beech. I am a woman of simple tastes. I enjoy reading, thinking, and doing my taxes. I entered this competition because I want a serious relationship. I want a commitment. The last man I dated was too whimsical. He wanted to go on dates that had no plan. No end goal. Sometimes we would just end up wandering the streets after dinner. He called it a \"\"walk\"\". A \"\"walk\"\" with no destination. Can you imagine? I like every action I take to have a measurable effect. When I see a movie, I like to walk away with insights that I did not have before. When I take a bike ride, there better be a worthy destination at the end of the bike path. Jay seems frivolous at times. This worries me. However, it is my staunch belief that one does not make and keep money without having a modicum of discipline. As such, I am hopeful. I will now list three things I cannot live without. Water. Emery boards. Dogs. Thank you for the opportunity to introduce myself. I look forward to the competition.\"\n",
    "gregg_t_fishy_intro = \"A most good day to you all, I am Gregg T Fishy, of the Fishy Enterprise fortune. I am 37 years young, an adventurous spirit and I've never lost my sense of childlike wonder. I do love to be in the backyard gardening and I have the most extraordinary time when I'm fishing. Fishing for what, you might find yourself asking? Why, I happen to always be fishing for compliments of course! I have a stunning pair of radiant blue eyes that will pierce the soul of anyone who dare gaze upon my countenance. I quite enjoy going on long jaunts through garden paths and short walks through greenhouses. I hope that Jay will be as absolutely interesting as he appears on the television, I find that he has some of the most curious tastes in style and humor. When I'm out and about I quite enjoy hearing tales that instill in my heart of hearts the fascination that beguiles my every day life, every fiber of my being scintillates and vascillates with extreme pleasure during one of these charming anecdotes and significantly pleases my beautiful personage. I cannot wait to enjoy being on the television program A Jay To Remember, it certainly seems like a grand time to explore life and love.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The First Indicator: Sentence Length\n",
    "\n",
    "Perhaps some meaningful data can first be gleaned from these text examples if we measure how long the average sentence length is. Different authors have different patterns of written speech, so this could be very useful in tracking down the killer.\n",
    "\n",
    "Write a function `get_average_sentence_length` that takes some `text` as an argument. This function should return the average length of a sentence in the text.\n",
    "\n",
    "Hint (highlight this hint in order to reveal it): \n",
    "<font color=\"white\">Use your knowledge of _string methods_ to create a list of all of the sentences in a text, called **sentences_in_text**. \n",
    "Further break up each **sentences_in_text** into a list of words and save the _length_ of that list of words to a new list that contains all the sentence lengths, called **sentence_lengths**. Take the average of all of the sentence lengths by adding them all together and dividing by the number of sentences (which should be the same as the length of the **sentence_lengths**).\n",
    "\n",
    "Remember sentences can end with more than one kind of punctuation, you might find it easiest to use **.replace()** so you only have to split on one punctuation mark. Remember **.replace()** doesn't modify the string itself, it returns a new string!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE SENTENCE LENGTHS\n",
      "Murder Note :  112.8\n",
      "Greg's Intro :  117.3\n",
      "Lily's Intro :  79.79\n",
      "Myrtle's Intro :  35.18\n"
     ]
    }
   ],
   "source": [
    "#I did start off writting a function called get_average_sentence_lenght.\n",
    "#I decided to take it a step further and create a class to manage the data. \n",
    "#I should have read to the next step.\n",
    "\n",
    "class splitText:\n",
    "    def __init__(self, text, name):\n",
    "        self.name = name\n",
    "        self.text = str(text)\n",
    "        self.list = text.replace(\". \", \".& \").replace(\"! \", \"!& \").replace(\"? \", \"?& \").split(\"& \")\n",
    "        self.numSentences = len(self.list)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "        \n",
    "    def sentenceList(self):\n",
    "        return self.list\n",
    "    \n",
    "    def total_len(self):\n",
    "        total_len = 0\n",
    "        for sentence in self.list:\n",
    "            total_len += len(sentence)\n",
    "        return round(total_len,2)\n",
    "    \n",
    "    def get_average_sentence_length(self):\n",
    "        total_len = 0\n",
    "        for sentence in self.list:\n",
    "            total_len += len(sentence)\n",
    "        return round(total_len/self.numSentences,2)\n",
    "        \n",
    "\n",
    "murder_note_split = splitText(murder_note, \"Murder Note\")\n",
    "lily_trebuchet_split = splitText(lily_trebuchet_intro, \"Lily's Intro\")\n",
    "myrtle_beech_intro_split = splitText(myrtle_beech_intro, \"Myrtle's Intro\")\n",
    "greg_fishy_intro_split = splitText(gregg_t_fishy_intro, \"Greg's Intro\")\n",
    "\n",
    "\n",
    "print (\"AVERAGE SENTENCE LENGTHS\")\n",
    "print(murder_note_split, \": \", murder_note_split.get_average_sentence_length())\n",
    "print(greg_fishy_intro_split, \": \", greg_fishy_intro_split.get_average_sentence_length())\n",
    "print(lily_trebuchet_split, \": \", lily_trebuchet_split.get_average_sentence_length())\n",
    "print(myrtle_beech_intro_split, \": \", myrtle_beech_intro_split.get_average_sentence_length())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Definition for Our Model\n",
    "\n",
    "Now that we have a metric we want to save and data that is coupled with that metric, it might be time to create our data type. Let's define a class called `TextSample` with a constructor. The constructor should take two arguments: `text` and `author`. `text` should be saved as `self.raw_text`. Call `get_average_sentence_length` with the raw text and save it to `self.average_sentence_length`. You should save the author of the text as `self.author`.\n",
    "\n",
    "Additionally, define a string representation for the model. If you print a `TextSample` it should render:\n",
    " - The author's name\n",
    " - The average sentence length\n",
    " \n",
    "This will be your main class for the problem at hand. All later instruction to update `TextSample` should be done in the code block below. After updating `TextSample`, click on the `Cell` option in the Jupyter Notebook main menu above, then click `Run All` to rerun the cells from top to bottom. If you need to restart your Jupyter Notebook either run the cells below first or move the `TextSample` class definition & instantiation cells to the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSample:\n",
    "    def __init__(self, text, author):\n",
    "        self.author = author\n",
    "        self.raw_text = str(text)\n",
    "        self.sentence_list = text.replace(\". \", \".& \").replace(\"! \", \"!& \").replace(\"? \", \"?& \").split(\"& \")\n",
    "        \n",
    "        self.numSentences = len(self.sentence_list)\n",
    "        self.average_sentence_length = self.get_average_sentence_length()\n",
    "        self.prepared_text = self.prepare_text(self.raw_text)\n",
    "        self.word_count_frequency = self.build_frequency_table(self.prepared_text)\n",
    "        self.ngram_list = self.ngram_creator(self.prepared_text)\n",
    "        self.ngram_frequency = self.build_frequency_table(self.ngram_list)\n",
    "        \n",
    "    \n",
    "    def total_len(self):\n",
    "        total_len = 0\n",
    "        for sentence in self.list:\n",
    "            total_len += len(sentence)\n",
    "            print (total_len)\n",
    "        return round(total_len,2)\n",
    "    \n",
    "    def get_average_sentence_length(self):\n",
    "        total_len = 0\n",
    "        for sentence in self.sentence_list:\n",
    "            total_len += len(sentence)        \n",
    "        return round(total_len/self.numSentences,2)\n",
    "    \n",
    "    def prepare_text(self, text):\n",
    "        clean_text = text.lower().replace(\",\",\"\").replace(\".\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "        text_list = clean_text.split()\n",
    "        return text_list\n",
    "    \n",
    "    def build_frequency_table(self, corpus):\n",
    "    \n",
    "        frequency_list = [\n",
    "            corpus.count(word) for word in corpus\n",
    "        ]\n",
    "        frequency_data = {key:value for key, value in zip(corpus, frequency_list)}\n",
    "        sorted_data = sorted(frequency_data.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        frequency_table = {key:value for key,value in sorted_data}\n",
    "        return frequency_table\n",
    "    \n",
    "    def ngram_creator(self,text_list):\n",
    "        ngram_list = []\n",
    "        for i in range(len(text_list)-1):\n",
    "            ngram_list.append(\"{word} {next_word}\".format(word=text_list[i], next_word=text_list[i+1]))\n",
    "\n",
    "        return ngram_list\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return_string = \"{author}, Average Sentence Length:{asl}.\".format(author=self.author, asl=self.average_sentence_length)\n",
    "        return return_string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our TextSample Instances\n",
    "\n",
    "Now create a `TextSample` object for each of the samples of text that we have.\n",
    " - `murderer_sample` for the murderer's note.\n",
    " - `lily_sample` for Lily Trebuchet's note.\n",
    " - `myrtle_sample` for Myrtle Beech's note.\n",
    " - `gregg_sample` for Gregg T Fishy's note.\n",
    " \n",
    "Print out each one after instantiating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murder, Average Sentence Length:112.8.\n",
      "Fishy, Average Sentence Length:117.3.\n",
      "Trebuchet, Average Sentence Length:79.79.\n",
      "Beech, Average Sentence Length:35.18.\n"
     ]
    }
   ],
   "source": [
    "murderer_sample = TextSample(murder_note, \"Murder\")\n",
    "lily_sample = TextSample(lily_trebuchet_intro, \"Trebuchet\")\n",
    "myrtle_sample = TextSample(myrtle_beech_intro, \"Beech\")\n",
    "gregg_sample = TextSample(gregg_t_fishy_intro, \"Fishy\")\n",
    "\n",
    "print (murderer_sample)\n",
    "print (gregg_sample)\n",
    "print (lily_sample)\n",
    "print (myrtle_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Our Data\n",
    "\n",
    "We want to compare the word choice and usage between the samples, but sentences make our text data fairly messy. In order to analyze the different messages fairly, we'll need to remove all the punctuation and uppercase letters from the samples.\n",
    "\n",
    "Create a function called `prepare_text` that takes a single parameter `text`, makes the text entirely lowercase, removes all the punctuation and returns a list of the words in the text in order.\n",
    "\n",
    "For example: `\"Where did you go, friend? We nearly saw each other.\"` would become `['where', 'did', 'you', 'go', 'friend', 'we', 'nearly', 'saw', 'each', 'other']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function called prepare_text that takes a single parameter text\n",
    "#make the text entirely lowercase\n",
    "#remove all the punctuation\n",
    "#return a list of the words in the text in order.\n",
    "\n",
    "\n",
    "\n",
    "def prepare_text(text):\n",
    "    clean_text = text.lower().replace(\",\",\"\").replace(\".\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "    text_list = clean_text.split()\n",
    "    return text_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the constructor for `TextSample` to save the prepared text as `self.prepared_text`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (murderer_sample.prepared_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Frequency Table\n",
    "\n",
    "Now we want to see which words were most frequently used in each of the samples. Create a function called `build_frequency_table`. It takes in a list called `corpus` and creates a dictionary called `frequency_table`. For every element in `corpus` the value `frequency_table[element]` should be equal to the number of times that element appears in `corpus`. For example the input `['do', 'you', 'see', 'what', 'i', 'see']` would create the frequency table `{'what': 1, 'you': 1, 'see' 2, 'i': 1}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function called `build_frequency_table`. \n",
    "#It takes in a list called `corpus` and creates a dictionary called `frequency_table`. \n",
    "#For every element in `corpus` the value `frequency_table[element]` should be equal to the number of times that element appears in `corpus`. \n",
    "#For example the input `['do', 'you', 'see', 'what', 'i', 'see']` would create the frequency table `{'what': 1, 'you': 1, 'see' 2, 'i': 1}`.\n",
    "\n",
    "\n",
    "def build_frequency_table(corpus):\n",
    "    \n",
    "    frequency_list = [\n",
    "        corpus.count(word) for word in corpus\n",
    "    ]\n",
    "        \n",
    "    frequency_data = {key:value for key, value in zip(corpus, frequency_list)}\n",
    "    \n",
    "    sorted_data = sorted(frequency_data.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    frequency_table = {key:value for key,value in sorted_data}\n",
    "    \n",
    "    return frequency_table\n",
    "\n",
    "#print (build_frequency_table(murderer_sample.prepared_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Second Indicator: Favorite Words\n",
    "\n",
    "Use `build_frequency_table` with the prepared text to create a frequency table that counts how frequently all the words in each text sample appears. Call these functions in the constructor for `TextSample` and assign the word frequency table to a value called `self.word_count_frequency`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murder, Average Sentence Length:112.8.\n",
      "{'he': 13, 'the': 12, 'a': 11, 'to': 10, 'was': 10, 'and': 9, 'all': 9, 'him': 8, 'of': 8, 'that': 7, 'i': 7, 'you': 6, 'his': 5, 'like': 5, 'us': 5, 'know': 3, 'things': 3, 'crying': 3, 'with': 3, 'live': 3, 'in': 3, 'what': 3, 'is': 3, 'showed': 3, 'but': 2, 'jay': 2, 'this': 2, 'contest': 2, 'massive': 2, 'for': 2, 'around': 2, 'cruel': 2, 'she': 2, 'go': 2, 'her': 2, 'night': 2, 'up': 2, 'we': 2, 'never': 2, 'on': 2, 'stick': 2, 'terrible': 2, 'would': 2, 'who': 2, 'any': 2, 'got': 2, 'deserved': 2, \"wasn't\": 2, 'going': 2, 'sick': 2, 'may': 1, 'call': 1, 'me': 1, 'heartless': 1, 'killer': 1, 'monster': 1, 'murderer': 1, \"i'm\": 1, 'still': 1, 'nothing': 1, 'compared': 1, 'villian': 1, 'whole': 1, 'sham': 1, 'an': 1, 'elaborate': 1, 'plot': 1, 'shame': 1, 'contestants': 1, 'feed': 1, \"jay's\": 1, 'ego': 1, 'sure': 1, 'think': 1, \"you've\": 1, 'seen': 1, 'smiling': 1, 'cameras': 1, 'laughing': 1, 'joking': 1, 'telling': 1, 'stories': 1, 'waving': 1, 'money': 1, 'prop': 1, 'off': 1, 'camera': 1, 'sinister': 1, 'beast': 1, 'taskmaster': 1, 'treated': 1, 'slaves': 1, 'cattle': 1, 'animals': 1, 'do': 1, 'remember': 1, 'lindsay': 1, 'first': 1, 'called': 1, 'such': 1, 'horrible': 1, 'cried': 1, 'keeping': 1, 'more': 1, 'broke': 1, 'words': 1, 'miss': 1, 'my': 1, 'former': 1, 'cast': 1, 'members': 1, 'them': 1, 'very': 1, 'much': 1, 'had': 1, 'home': 1, 'power': 1, 'deal': 1, 'crazy': 1, 'demands': 1, 'did': 1, 'prize': 1, \"isn't\": 1, 'real': 1, 'intended': 1, 'marry': 1, 'one': 1, 'carrot': 1, 'gone': 1, 'left': 1, 'told': 1, 'last': 1, 'were': 1, 'disappointment': 1, 'none': 1, 'ever': 1, 'amount': 1, 'anything': 1, 'regardless': 1, 'won': 1, 'speak': 1, 'again': 1, \"it's\": 1, 'definitely': 1, 'can': 1, 'feel': 1, 'your': 1, 'gut': 1, 'how': 1, 'wrong': 1, 'well': 1, 'right': 1, 'person': 1, 'am': 1, 'be': 1, 'pushed': 1, 'longer': 1, 'let': 1, 'pretending': 1, 'some': 1, 'saint': 1, 'when': 1, 'twisted': 1, 'man': 1, 'every': 1, 'bit': 1, 'fans': 1, 'need': 1, 'stacksby': 1, 'vile': 1, 'amalgamation': 1, 'evil': 1, 'bad': 1, 'world': 1, 'better': 1, 'place': 1, 'without': 1}\n",
      " \n",
      "Trebuchet, Average Sentence Length:79.79.\n",
      "{'i': 11, 'and': 10, 'the': 10, 'you': 9, 'is': 9, 'to': 8, 'of': 7, 'a': 6, 'with': 5, 'my': 5, 'for': 4, 'what': 4, 'your': 4, 'one': 4, 'but': 4, 'think': 4, 'okay': 4, 'it': 4, 'favorite': 3, 'have': 3, 'handsome': 3, 'he': 3, 'in': 3, 'all': 3, 'love': 2, 'so': 2, 'they': 2, 'are': 2, 'dishes': 2, 'definitely': 2, 'them': 2, \"that's\": 2, 'hard': 2, \"i'll\": 2, 'go': 2, 'man': 2, 'do': 2, 'him': 2, 'question': 2, 'well': 2, 'know': 2, 'music': 2, 'can': 2, 'first': 2, 'beautiful': 2, 'pasta': 2, 'me': 2, 'be': 2, 'hi': 1, \"i'm\": 1, 'lily': 1, 'trebuchet': 1, 'from': 1, 'east': 1, 'egg': 1, 'long': 1, 'island': 1, 'cats': 1, 'hiking': 1, 'curling': 1, 'up': 1, 'under': 1, 'warm': 1, 'blanket': 1, 'book': 1, 'gave': 1, 'this': 1, 'little': 1, 'questionnaire': 1, 'use': 1, 'our': 1, 'bios': 1, 'lets': 1, 'get': 1, 'started': 1, 'some': 1, 'least': 1, 'household': 1, 'chores': 1, 'oh': 1, 'yes': 1, \"it's\": 1, 'just': 1, 'hate': 1, 'doing': 1, \"don't\": 1, 'who': 1, 'actor': 1, 'why': 1, 'hmm': 1, 'recently': 1, 'michael': 1, 'b': 1, 'jordan': 1, 'every': 1, 'bit': 1, 'that': 1, 'remember': 1, 'seeing': 1, 'shirtless': 1, \"can't\": 1, 'believe': 1, 'does': 1, 'cameras': 1, 'next': 1, 'perfect': 1, 'date': 1, 'starts': 1, 'nice': 1, 'dinner': 1, 'at': 1, 'delicious': 1, 'small': 1, 'restaurant': 1, 'like': 1, 'those': 1, 'places': 1, 'where': 1, 'owner': 1, 'back': 1, 'comes': 1, 'out': 1, 'talk': 1, 'ask': 1, 'how': 1, 'meal': 1, 'was': 1, 'form': 1, 'art': 1, 'another': 1, 'feel': 1, 'whole': 1, 'body': 1, 'electrifying': 1, 'best': 1, 'dance': 1, 'final': 1, \"let's\": 1, 'see': 1, 'three': 1, 'things': 1, 'cannot': 1, 'live': 1, 'without': 1, 'off': 1, 'cat': 1, 'jerry': 1, 'heart': 1, 'spirit': 1, 'animal': 1, 'second': 1, 'third': 1, 'family': 1, 'very': 1, 'much': 1, 'support': 1, 'everything': 1, 'jay': 1, 'stacksby': 1, 'us': 1, 'want': 1, 'walk': 1, 'down': 1, 'aisle': 1, 'might': 1, 'truly': 1, 'bio': 1, 'hope': 1, 'fun': 1, 'watching': 1, 'show': 1}\n",
      " \n",
      "Beech, Average Sentence Length:35.18.\n",
      "{'i': 16, 'a': 10, 'the': 6, 'to': 5, 'my': 3, 'of': 3, 'that': 3, 'no': 3, 'end': 3, 'walk': 3, 'myrtle': 2, 'am': 2, 'and': 2, 'this': 2, 'competition': 2, 'want': 2, 'he': 2, 'it': 2, 'with': 2, 'destination': 2, 'you': 2, 'like': 2, 'take': 2, 'have': 2, 'when': 2, 'not': 2, 'bike': 2, 'at': 2, 'without': 2, 'salutations': 1, 'name': 1, 'beech': 1, 'woman': 1, 'simple': 1, 'tastes': 1, 'enjoy': 1, 'reading': 1, 'thinking': 1, 'doing': 1, 'taxes': 1, 'entered': 1, 'because': 1, 'serious': 1, 'relationship': 1, 'commitment': 1, 'last': 1, 'man': 1, 'dated': 1, 'was': 1, 'too': 1, 'whimsical': 1, 'wanted': 1, 'go': 1, 'on': 1, 'dates': 1, 'had': 1, 'plan': 1, 'goal': 1, 'sometimes': 1, 'we': 1, 'would': 1, 'just': 1, 'up': 1, 'wandering': 1, 'streets': 1, 'after': 1, 'dinner': 1, 'called': 1, 'can': 1, 'imagine': 1, 'every': 1, 'action': 1, 'measurable': 1, 'effect': 1, 'see': 1, 'movie': 1, 'away': 1, 'insights': 1, 'did': 1, 'before': 1, 'ride': 1, 'there': 1, 'better': 1, 'be': 1, 'worthy': 1, 'path': 1, 'jay': 1, 'seems': 1, 'frivolous': 1, 'times': 1, 'worries': 1, 'me': 1, 'however': 1, 'is': 1, 'staunch': 1, 'belief': 1, 'one': 1, 'does': 1, 'make': 1, 'keep': 1, 'money': 1, 'having': 1, 'modicum': 1, 'discipline': 1, 'as': 1, 'such': 1, 'hopeful': 1, 'will': 1, 'now': 1, 'list': 1, 'three': 1, 'things': 1, 'cannot': 1, 'live': 1, 'water': 1, 'emery': 1, 'boards': 1, 'dogs': 1, 'thank': 1, 'for': 1, 'opportunity': 1, 'introduce': 1, 'myself': 1, 'look': 1, 'forward': 1}\n",
      " \n",
      "Fishy, Average Sentence Length:117.3.\n",
      "{'i': 11, 'of': 9, 'the': 8, 'and': 8, 'to': 6, 'my': 6, 'that': 5, 'a': 4, 'most': 3, 'be': 3, 'in': 3, 'fishing': 3, 'enjoy': 3, 'on': 3, 'day': 2, 'you': 2, 'am': 2, 'fishy': 2, 'love': 2, 'have': 2, 'time': 2, 'when': 2, \"i'm\": 2, 'for': 2, 'find': 2, 'will': 2, 'quite': 2, 'through': 2, 'jay': 2, 'as': 2, 'he': 2, 'television': 2, 'every': 2, 'life': 2, 'being': 2, 'good': 1, 'all': 1, 'gregg': 1, 't': 1, 'enterprise': 1, 'fortune': 1, '37': 1, 'years': 1, 'young': 1, 'an': 1, 'adventurous': 1, 'spirit': 1, \"i've\": 1, 'never': 1, 'lost': 1, 'sense': 1, 'childlike': 1, 'wonder': 1, 'do': 1, 'backyard': 1, 'gardening': 1, 'extraordinary': 1, 'what': 1, 'might': 1, 'yourself': 1, 'asking': 1, 'why': 1, 'happen': 1, 'always': 1, 'compliments': 1, 'course': 1, 'stunning': 1, 'pair': 1, 'radiant': 1, 'blue': 1, 'eyes': 1, 'pierce': 1, 'soul': 1, 'anyone': 1, 'who': 1, 'dare': 1, 'gaze': 1, 'upon': 1, 'countenance': 1, 'going': 1, 'long': 1, 'jaunts': 1, 'garden': 1, 'paths': 1, 'short': 1, 'walks': 1, 'greenhouses': 1, 'hope': 1, 'absolutely': 1, 'interesting': 1, 'appears': 1, 'has': 1, 'some': 1, 'curious': 1, 'tastes': 1, 'style': 1, 'humor': 1, 'out': 1, 'about': 1, 'hearing': 1, 'tales': 1, 'instill': 1, 'heart': 1, 'hearts': 1, 'fascination': 1, 'beguiles': 1, 'fiber': 1, 'scintillates': 1, 'vascillates': 1, 'with': 1, 'extreme': 1, 'pleasure': 1, 'during': 1, 'one': 1, 'these': 1, 'charming': 1, 'anecdotes': 1, 'significantly': 1, 'pleases': 1, 'beautiful': 1, 'personage': 1, 'cannot': 1, 'wait': 1, 'program': 1, 'remember': 1, 'it': 1, 'certainly': 1, 'seems': 1, 'like': 1, 'grand': 1, 'explore': 1}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Call these functions in the constructor for TextSample\n",
    "#assign the word frequency table to a value called self.word_count_frequency.\n",
    "#Use build_frequency_table with the prepared text to create a frequency table\n",
    "#count how frequently all the words in each text sample appears.\n",
    "\n",
    "print (murderer_sample)\n",
    "print (murderer_sample.word_count_frequency)\n",
    "print (\" \")\n",
    "\n",
    "print (lily_sample)\n",
    "print (lily_sample.word_count_frequency)\n",
    "print (\" \")\n",
    "\n",
    "print (myrtle_sample)\n",
    "print (myrtle_sample.word_count_frequency)\n",
    "print (\" \")\n",
    "\n",
    "print (gregg_sample)\n",
    "print (gregg_sample.word_count_frequency)\n",
    "print (\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Third Indicator: N-Grams\n",
    "\n",
    "An <a href='https://en.wikipedia.org/wiki/N-gram' target=\"_blank\">n-gram</a> is a text analysis technique used for pattern recognition and applicable throughout lingusitics. We're going to use n-grams to find who uses similar word-pairs to the murderer, and we think it's going to make our evidence strong enough to conclusively find the killer.\n",
    "\n",
    "Create a function called `ngram_creator` that takes a parameter `text_list`, a treated in-order list of the words in a text sample. `ngram_creator` should return a list of all adjacent pairs of words, styled as strings with a space in the center.\n",
    "\n",
    "For instance, calling `ngram_creator` with the input `['what', 'in', 'the', 'world', 'is', 'going', 'on']`\n",
    "Should produce the output `['what in', 'in the', 'the world', 'world is', 'is going', 'going on']`.\n",
    "\n",
    "These are two-word n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function called ngram_creator-\n",
    "#that takes a parameter text_list, a treated in-order list of the words in a text sample (prepared_text) -\n",
    "#ngram_creator should return a list of all adjacent pairs of words, styled as strings with a space in the center.-\n",
    "\n",
    "def ngram_creator(text_list):\n",
    "    ngram_list = []\n",
    "    for i in range(len(text_list)-1):\n",
    "        ngram_list.append(\"{word} {next_word}\".format(word=text_list[i], next_word=text_list[i+1]))\n",
    "    \n",
    "    return ngram_list\n",
    "\n",
    "#print (ngram_creator(murderer_sample.prepared_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `ngram_creator` along with the prepared text to create a list of all the two-word ngrams in each `TextSample`. Use `build_frequency_table` to tabulate the frequency of each ngram. In the constructor for `TextSample` save this frequency table as `self.ngram_frequency`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murderer's Sample\n",
      "{'of us': 4, 'was a': 3, 'he was': 3, 'i showed': 3, 'showed him': 3, 'you know': 2, 'all of': 2, 'with his': 2, 'live in': 2, 'in his': 2, 'he got': 2, 'what he': 2, \"i wasn't\": 2, \"wasn't going\": 2, 'going to': 2, 'is a': 2, 'you may': 1, 'may call': 1, 'call me': 1, 'me heartless': 1, 'heartless a': 1, 'a killer': 1, 'killer a': 1, 'a monster': 1, 'monster a': 1, 'a murderer': 1, 'murderer but': 1, \"but i'm\": 1, \"i'm still\": 1, 'still nothing': 1, 'nothing compared': 1, 'compared to': 1, 'to the': 1, 'the villian': 1, 'villian that': 1, 'that jay': 1, 'jay was': 1, 'was this': 1, 'this whole': 1, 'whole contest': 1, 'contest was': 1, 'a sham': 1, 'sham an': 1, 'an elaborate': 1, 'elaborate plot': 1, 'plot to': 1, 'to shame': 1, 'shame the': 1, 'the contestants': 1, 'contestants and': 1, 'and feed': 1, \"feed jay's\": 1, \"jay's massive\": 1, 'massive massive': 1, 'massive ego': 1, 'ego sure': 1, 'sure you': 1, 'you think': 1, 'think you': 1, 'know him': 1, \"him you've\": 1, \"you've seen\": 1, 'seen him': 1, 'him smiling': 1, 'smiling for': 1, 'for the': 1, 'the cameras': 1, 'cameras laughing': 1, 'laughing joking': 1, 'joking telling': 1, 'telling stories': 1, 'stories waving': 1, 'waving his': 1, 'his money': 1, 'money around': 1, 'around like': 1, 'like a': 1, 'a prop': 1, 'prop but': 1, 'but off': 1, 'off camera': 1, 'camera he': 1, 'a sinister': 1, 'sinister beast': 1, 'beast a': 1, 'a cruel': 1, 'cruel cruel': 1, 'cruel taskmaster': 1, 'taskmaster he': 1, 'he treated': 1, 'treated all': 1, 'us like': 1, 'like slaves': 1, 'slaves like': 1, 'like cattle': 1, 'cattle like': 1, 'like animals': 1, 'animals do': 1, 'do you': 1, 'you remember': 1, 'remember lindsay': 1, 'lindsay she': 1, 'she was': 1, 'was the': 1, 'the first': 1, 'first to': 1, 'to go': 1, 'go he': 1, 'he called': 1, 'called her': 1, 'her such': 1, 'such horrible': 1, 'horrible things': 1, 'things that': 1, 'that she': 1, 'she cried': 1, 'cried all': 1, 'all night': 1, 'night keeping': 1, 'keeping up': 1, 'up all': 1, 'all up': 1, 'up crying': 1, 'crying crying': 1, 'crying and': 1, 'and more': 1, 'more crying': 1, 'crying he': 1, 'he broke': 1, 'broke her': 1, 'her with': 1, 'his words': 1, 'words i': 1, 'i miss': 1, 'miss my': 1, 'my former': 1, 'former cast': 1, 'cast members': 1, 'members all': 1, 'of them': 1, 'them very': 1, 'very much': 1, 'much and': 1, 'and we': 1, 'we had': 1, 'had to': 1, 'to live': 1, 'live with': 1, 'with him': 1, 'him live': 1, 'his home': 1, 'home live': 1, 'his power': 1, 'power deal': 1, 'deal with': 1, 'his crazy': 1, 'crazy demands': 1, 'demands and': 1, 'and for': 1, 'for what': 1, 'what did': 1, 'did you': 1, 'know that': 1, 'that the': 1, 'the prize': 1, \"prize isn't\": 1, \"isn't real\": 1, 'real he': 1, 'he never': 1, 'never intended': 1, 'intended to': 1, 'to marry': 1, 'marry one': 1, 'one of': 1, 'us the': 1, 'the carrot': 1, 'carrot on': 1, 'on the': 1, 'the stick': 1, 'stick was': 1, 'was gone': 1, 'gone all': 1, 'all that': 1, 'that was': 1, 'was left': 1, 'left was': 1, 'was stick': 1, 'stick he': 1, 'he told': 1, 'told us': 1, 'us last': 1, 'last night': 1, 'night that': 1, 'that we': 1, 'we were': 1, 'were all': 1, 'all a': 1, 'a terrible': 1, 'terrible terrible': 1, 'terrible disappointment': 1, 'disappointment and': 1, 'and none': 1, 'none of': 1, 'us would': 1, 'would ever': 1, 'ever amount': 1, 'amount to': 1, 'to anything': 1, 'anything and': 1, 'and that': 1, 'that regardless': 1, 'regardless of': 1, 'of who': 1, 'who won': 1, 'won the': 1, 'the contest': 1, 'contest he': 1, 'he would': 1, 'would never': 1, 'never speak': 1, 'speak to': 1, 'to any': 1, 'any of': 1, 'us again': 1, \"again it's\": 1, \"it's definitely\": 1, 'definitely the': 1, 'the things': 1, 'things like': 1, 'like this': 1, 'this you': 1, 'you can': 1, 'can feel': 1, 'feel in': 1, 'in your': 1, 'your gut': 1, 'gut how': 1, 'how wrong': 1, 'wrong he': 1, 'he is': 1, 'is well': 1, 'well i': 1, 'him he': 1, 'got what': 1, 'he deserved': 1, 'deserved all': 1, 'all right': 1, 'right i': 1, 'him i': 1, 'him the': 1, 'the person': 1, 'person i': 1, 'i am': 1, 'am i': 1, 'to be': 1, 'be pushed': 1, 'pushed around': 1, 'around any': 1, 'any longer': 1, 'longer and': 1, 'and i': 1, 'to let': 1, 'let him': 1, 'him go': 1, 'go on': 1, 'on pretending': 1, 'pretending that': 1, 'that he': 1, 'was some': 1, 'some saint': 1, 'saint when': 1, 'when all': 1, 'all he': 1, 'was was': 1, 'a sick': 1, 'sick sick': 1, 'sick twisted': 1, 'twisted man': 1, 'man who': 1, 'who deserved': 1, 'deserved every': 1, 'every bit': 1, 'bit of': 1, 'of what': 1, 'got the': 1, 'the fans': 1, 'fans need': 1, 'need to': 1, 'to know': 1, 'know jay': 1, 'jay stacksby': 1, 'stacksby is': 1, 'a vile': 1, 'vile amalgamation': 1, 'amalgamation of': 1, 'of all': 1, 'all things': 1, 'things evil': 1, 'evil and': 1, 'and bad': 1, 'bad and': 1, 'and the': 1, 'the world': 1, 'world is': 1, 'a better': 1, 'better place': 1, 'place without': 1, 'without him': 1}\n",
      "\n",
      "Lily's Sample\n",
      "{'i think': 4, 'but i': 3, 'i love': 2, 'with a': 2, 'what are': 2, 'is your': 2, 'hard one': 2, 'one but': 2, \"i'll have\": 2, 'have to': 2, 'to go': 2, 'go with': 2, 'for the': 2, 'you can': 2, 'is my': 2, 'all of': 2, 'be the': 2, \"hi i'm\": 1, \"i'm lily\": 1, 'lily trebuchet': 1, 'trebuchet from': 1, 'from east': 1, 'east egg': 1, 'egg long': 1, 'long island': 1, 'island i': 1, 'love cats': 1, 'cats hiking': 1, 'hiking and': 1, 'and curling': 1, 'curling up': 1, 'up under': 1, 'under a': 1, 'a warm': 1, 'warm blanket': 1, 'blanket with': 1, 'a book': 1, 'book so': 1, 'so they': 1, 'they gave': 1, 'gave this': 1, 'this little': 1, 'little questionnaire': 1, 'questionnaire to': 1, 'to use': 1, 'use for': 1, 'for our': 1, 'our bios': 1, 'bios so': 1, 'so lets': 1, 'lets get': 1, 'get started': 1, 'started what': 1, 'are some': 1, 'some of': 1, 'of my': 1, 'my least': 1, 'least favorite': 1, 'favorite household': 1, 'household chores': 1, 'chores dishes': 1, 'dishes oh': 1, 'oh yes': 1, \"yes it's\": 1, \"it's definitely\": 1, 'definitely the': 1, 'the dishes': 1, 'dishes i': 1, 'i just': 1, 'just hate': 1, 'hate doing': 1, 'doing them': 1, \"them don't\": 1, \"don't you\": 1, 'you who': 1, 'who is': 1, 'your favorite': 1, 'favorite actor': 1, 'actor and': 1, 'and why': 1, 'why hmm': 1, \"hmm that's\": 1, \"that's a\": 1, 'a hard': 1, 'think recently': 1, \"recently i'll\": 1, 'with michael': 1, 'michael b': 1, 'b jordan': 1, 'jordan every': 1, 'every bit': 1, 'bit of': 1, 'of that': 1, 'that man': 1, 'man is': 1, 'is handsome': 1, 'handsome handsome': 1, 'handsome do': 1, 'do you': 1, 'you remember': 1, 'remember seeing': 1, 'seeing him': 1, 'him shirtless': 1, 'shirtless i': 1, \"i can't\": 1, \"can't believe\": 1, 'believe what': 1, 'what he': 1, 'he does': 1, 'does for': 1, 'the cameras': 1, 'cameras okay': 1, 'okay okay': 1, 'okay next': 1, 'next question': 1, 'question what': 1, 'what is': 1, 'your perfect': 1, 'perfect date': 1, 'date well': 1, 'well it': 1, 'it starts': 1, 'starts with': 1, 'a nice': 1, 'nice dinner': 1, 'dinner at': 1, 'at a': 1, 'a delicious': 1, 'delicious but': 1, 'but small': 1, 'small restaurant': 1, 'restaurant you': 1, 'you know': 1, 'know like': 1, 'like one': 1, 'one of': 1, 'of those': 1, 'those places': 1, 'places where': 1, 'where the': 1, 'the owner': 1, 'owner is': 1, 'is in': 1, 'in the': 1, 'the back': 1, 'back and': 1, 'and comes': 1, 'comes out': 1, 'out to': 1, 'to talk': 1, 'talk to': 1, 'to you': 1, 'you and': 1, 'and ask': 1, 'ask you': 1, 'you how': 1, 'how your': 1, 'your meal': 1, 'meal was': 1, 'was my': 1, 'my favorite': 1, 'favorite form': 1, 'form of': 1, 'of art': 1, 'art another': 1, 'another hard': 1, \"think i'll\": 1, 'with music': 1, 'music music': 1, 'music you': 1, 'can feel': 1, 'feel in': 1, 'in your': 1, 'your whole': 1, 'whole body': 1, 'body and': 1, 'and it': 1, 'it is': 1, 'is electrifying': 1, 'electrifying and': 1, 'and best': 1, 'best of': 1, 'of all': 1, 'all you': 1, 'can dance': 1, 'dance to': 1, 'to it': 1, 'it okay': 1, 'okay final': 1, 'final question': 1, \"question let's\": 1, \"let's see\": 1, 'see what': 1, 'are three': 1, 'three things': 1, 'things you': 1, 'you cannot': 1, 'cannot live': 1, 'live without': 1, 'without well': 1, 'well first': 1, 'first off': 1, 'off my': 1, 'my beautiful': 1, 'beautiful beautiful': 1, 'beautiful cat': 1, 'cat jerry': 1, 'jerry he': 1, 'he is': 1, 'my heart': 1, 'heart and': 1, 'and spirit': 1, 'spirit animal': 1, 'animal second': 1, 'second is': 1, 'is pasta': 1, 'pasta definitely': 1, 'definitely pasta': 1, 'pasta and': 1, 'and the': 1, 'the third': 1, 'third i': 1, 'think is': 1, 'my family': 1, 'family i': 1, 'love all': 1, 'of them': 1, 'them very': 1, 'very much': 1, 'much and': 1, 'and they': 1, 'they support': 1, 'support me': 1, 'me in': 1, 'in everything': 1, 'everything i': 1, 'i do': 1, 'do i': 1, 'i know': 1, 'know jay': 1, 'jay stacksby': 1, 'stacksby is': 1, 'is a': 1, 'a handsome': 1, 'handsome man': 1, 'man and': 1, 'and all': 1, 'of us': 1, 'us want': 1, 'want to': 1, 'to be': 1, 'the first': 1, 'first to': 1, 'to walk': 1, 'walk down': 1, 'down the': 1, 'the aisle': 1, 'aisle with': 1, 'with him': 1, 'him but': 1, 'think he': 1, 'he might': 1, 'might truly': 1, 'truly be': 1, 'the one': 1, 'one for': 1, 'for me': 1, 'me okay': 1, \"okay that's\": 1, \"that's it\": 1, 'it for': 1, 'the bio': 1, 'bio i': 1, 'i hope': 1, 'hope you': 1, 'you have': 1, 'have fun': 1, 'fun watching': 1, 'watching the': 1, 'the show': 1}\n",
      "\n",
      "Gregg's Sample\n",
      "{'i am': 2, 'of the': 2, 'i have': 2, 'the most': 2, \"when i'm\": 2, 'fishing for': 2, 'i quite': 2, 'quite enjoy': 2, 'on the': 2, 'the television': 2, 'a most': 1, 'most good': 1, 'good day': 1, 'day to': 1, 'to you': 1, 'you all': 1, 'all i': 1, 'am gregg': 1, 'gregg t': 1, 't fishy': 1, 'fishy of': 1, 'the fishy': 1, 'fishy enterprise': 1, 'enterprise fortune': 1, 'fortune i': 1, 'am 37': 1, '37 years': 1, 'years young': 1, 'young an': 1, 'an adventurous': 1, 'adventurous spirit': 1, 'spirit and': 1, \"and i've\": 1, \"i've never\": 1, 'never lost': 1, 'lost my': 1, 'my sense': 1, 'sense of': 1, 'of childlike': 1, 'childlike wonder': 1, 'wonder i': 1, 'i do': 1, 'do love': 1, 'love to': 1, 'to be': 1, 'be in': 1, 'in the': 1, 'the backyard': 1, 'backyard gardening': 1, 'gardening and': 1, 'and i': 1, 'have the': 1, 'most extraordinary': 1, 'extraordinary time': 1, 'time when': 1, \"i'm fishing\": 1, 'fishing fishing': 1, 'for what': 1, 'what you': 1, 'you might': 1, 'might find': 1, 'find yourself': 1, 'yourself asking': 1, 'asking why': 1, 'why i': 1, 'i happen': 1, 'happen to': 1, 'to always': 1, 'always be': 1, 'be fishing': 1, 'for compliments': 1, 'compliments of': 1, 'of course': 1, 'course i': 1, 'have a': 1, 'a stunning': 1, 'stunning pair': 1, 'pair of': 1, 'of radiant': 1, 'radiant blue': 1, 'blue eyes': 1, 'eyes that': 1, 'that will': 1, 'will pierce': 1, 'pierce the': 1, 'the soul': 1, 'soul of': 1, 'of anyone': 1, 'anyone who': 1, 'who dare': 1, 'dare gaze': 1, 'gaze upon': 1, 'upon my': 1, 'my countenance': 1, 'countenance i': 1, 'enjoy going': 1, 'going on': 1, 'on long': 1, 'long jaunts': 1, 'jaunts through': 1, 'through garden': 1, 'garden paths': 1, 'paths and': 1, 'and short': 1, 'short walks': 1, 'walks through': 1, 'through greenhouses': 1, 'greenhouses i': 1, 'i hope': 1, 'hope that': 1, 'that jay': 1, 'jay will': 1, 'will be': 1, 'be as': 1, 'as absolutely': 1, 'absolutely interesting': 1, 'interesting as': 1, 'as he': 1, 'he appears': 1, 'appears on': 1, 'television i': 1, 'i find': 1, 'find that': 1, 'that he': 1, 'he has': 1, 'has some': 1, 'some of': 1, 'most curious': 1, 'curious tastes': 1, 'tastes in': 1, 'in style': 1, 'style and': 1, 'and humor': 1, 'humor when': 1, \"i'm out\": 1, 'out and': 1, 'and about': 1, 'about i': 1, 'enjoy hearing': 1, 'hearing tales': 1, 'tales that': 1, 'that instill': 1, 'instill in': 1, 'in my': 1, 'my heart': 1, 'heart of': 1, 'of hearts': 1, 'hearts the': 1, 'the fascination': 1, 'fascination that': 1, 'that beguiles': 1, 'beguiles my': 1, 'my every': 1, 'every day': 1, 'day life': 1, 'life every': 1, 'every fiber': 1, 'fiber of': 1, 'of my': 1, 'my being': 1, 'being scintillates': 1, 'scintillates and': 1, 'and vascillates': 1, 'vascillates with': 1, 'with extreme': 1, 'extreme pleasure': 1, 'pleasure during': 1, 'during one': 1, 'one of': 1, 'of these': 1, 'these charming': 1, 'charming anecdotes': 1, 'anecdotes and': 1, 'and significantly': 1, 'significantly pleases': 1, 'pleases my': 1, 'my beautiful': 1, 'beautiful personage': 1, 'personage i': 1, 'i cannot': 1, 'cannot wait': 1, 'wait to': 1, 'to enjoy': 1, 'enjoy being': 1, 'being on': 1, 'television program': 1, 'program a': 1, 'a jay': 1, 'jay to': 1, 'to remember': 1, 'remember it': 1, 'it certainly': 1, 'certainly seems': 1, 'seems like': 1, 'like a': 1, 'a grand': 1, 'grand time': 1, 'time to': 1, 'to explore': 1, 'explore life': 1, 'life and': 1, 'and love': 1}\n",
      "\n",
      "Myrtle's Sample\n",
      "{'i am': 2, 'i want': 2, 'want a': 2, 'a walk': 2, 'i like': 2, 'i take': 2, 'when i': 2, 'salutations my': 1, 'my name': 1, 'name myrtle': 1, 'myrtle myrtle': 1, 'myrtle beech': 1, 'beech i': 1, 'am a': 1, 'a woman': 1, 'woman of': 1, 'of simple': 1, 'simple tastes': 1, 'tastes i': 1, 'i enjoy': 1, 'enjoy reading': 1, 'reading thinking': 1, 'thinking and': 1, 'and doing': 1, 'doing my': 1, 'my taxes': 1, 'taxes i': 1, 'i entered': 1, 'entered this': 1, 'this competition': 1, 'competition because': 1, 'because i': 1, 'a serious': 1, 'serious relationship': 1, 'relationship i': 1, 'a commitment': 1, 'commitment the': 1, 'the last': 1, 'last man': 1, 'man i': 1, 'i dated': 1, 'dated was': 1, 'was too': 1, 'too whimsical': 1, 'whimsical he': 1, 'he wanted': 1, 'wanted to': 1, 'to go': 1, 'go on': 1, 'on dates': 1, 'dates that': 1, 'that had': 1, 'had no': 1, 'no plan': 1, 'plan no': 1, 'no end': 1, 'end goal': 1, 'goal sometimes': 1, 'sometimes we': 1, 'we would': 1, 'would just': 1, 'just end': 1, 'end up': 1, 'up wandering': 1, 'wandering the': 1, 'the streets': 1, 'streets after': 1, 'after dinner': 1, 'dinner he': 1, 'he called': 1, 'called it': 1, 'it a': 1, 'walk a': 1, 'walk with': 1, 'with no': 1, 'no destination': 1, 'destination can': 1, 'can you': 1, 'you imagine': 1, 'imagine i': 1, 'like every': 1, 'every action': 1, 'action i': 1, 'take to': 1, 'to have': 1, 'have a': 1, 'a measurable': 1, 'measurable effect': 1, 'effect when': 1, 'i see': 1, 'see a': 1, 'a movie': 1, 'movie i': 1, 'like to': 1, 'to walk': 1, 'walk away': 1, 'away with': 1, 'with insights': 1, 'insights that': 1, 'that i': 1, 'i did': 1, 'did not': 1, 'not have': 1, 'have before': 1, 'before when': 1, 'take a': 1, 'a bike': 1, 'bike ride': 1, 'ride there': 1, 'there better': 1, 'better be': 1, 'be a': 1, 'a worthy': 1, 'worthy destination': 1, 'destination at': 1, 'at the': 1, 'the end': 1, 'end of': 1, 'of the': 1, 'the bike': 1, 'bike path': 1, 'path jay': 1, 'jay seems': 1, 'seems frivolous': 1, 'frivolous at': 1, 'at times': 1, 'times this': 1, 'this worries': 1, 'worries me': 1, 'me however': 1, 'however it': 1, 'it is': 1, 'is my': 1, 'my staunch': 1, 'staunch belief': 1, 'belief that': 1, 'that one': 1, 'one does': 1, 'does not': 1, 'not make': 1, 'make and': 1, 'and keep': 1, 'keep money': 1, 'money without': 1, 'without having': 1, 'having a': 1, 'a modicum': 1, 'modicum of': 1, 'of discipline': 1, 'discipline as': 1, 'as such': 1, 'such i': 1, 'am hopeful': 1, 'hopeful i': 1, 'i will': 1, 'will now': 1, 'now list': 1, 'list three': 1, 'three things': 1, 'things i': 1, 'i cannot': 1, 'cannot live': 1, 'live without': 1, 'without water': 1, 'water emery': 1, 'emery boards': 1, 'boards dogs': 1, 'dogs thank': 1, 'thank you': 1, 'you for': 1, 'for the': 1, 'the opportunity': 1, 'opportunity to': 1, 'to introduce': 1, 'introduce myself': 1, 'myself i': 1, 'i look': 1, 'look forward': 1, 'forward to': 1, 'to the': 1, 'the competition': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Murderer's Sample\")\n",
    "print (murderer_sample.ngram_frequency)\n",
    "print (\"\")\n",
    "print(\"Lily's Sample\")\n",
    "print (lily_sample.ngram_frequency)\n",
    "print (\"\")\n",
    "print(\"Gregg's Sample\")\n",
    "print (gregg_sample.ngram_frequency)\n",
    "print (\"\")\n",
    "print(\"Myrtle's Sample\")\n",
    "\n",
    "print (myrtle_sample.ngram_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Two Frequency Tables\n",
    "\n",
    "We want to know how similar two frequency tables are, let's write a function that computes the comparison between two frequency tables and scores them based on similarity.\n",
    "\n",
    "Write a function called `frequency_comparison` that takes two parameters, `table1` and `table2`. It should define two local variables, `appearances` and `mutual_appearances`. \n",
    "\n",
    "Iterate through `table1`'s keys and check if `table2` has the same key defined. If it is, compare the two values for the key -- the smaller value should get added to `mutual_appearances` and the larger should get added to `appearances`. If the key doesn't exist in `table2` the value for the key in `table1` should be added to `appearances`.\n",
    "\n",
    "Remember afterwards to iterate through all of `table2`'s keys that aren't in `table1` and add those to `appearances` as well.\n",
    "\n",
    "Return a frequency comparison score equal to the mutual appearances divided by the total appearances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Simiilarity: Lily Trebuchet\n",
      "0.23\n",
      " \n",
      "% Simiilarity: Myrtle Beech\n",
      "0.17\n",
      " \n",
      "% Simiilarity: Gregg Fishy\n",
      "0.16\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Write a function called frequency_comparison that takes two parameters, table1 and table2. \n",
    "\n",
    "def frequency_comparison(table1, table2): #word_count_frequency ['test':4, 'bold':3]\n",
    "    \n",
    "#It should define two local variables, appearances and mutual_appearances.\n",
    "    appearances = 0\n",
    "    mutual_appearances = 0\n",
    "\n",
    "#Iterate through table1's keys and check if table2 has the same key defined. \n",
    "\n",
    "    for key,value in table1.items():\n",
    "        #If it is, compare the two values for the key \n",
    "        if key in table2.keys():\n",
    "            #the smaller value should get added to mutual_appearance \n",
    "            mutual_appearances += min(table1.get(key), table2.get(key))\n",
    "            \n",
    "            #the larger should get added to appearances.\n",
    "            appearances += max(table1.get(key), table2.get(key))\n",
    "             \n",
    "        #If the key doesn't exist in table2 the value for the key in table1 should be added to appearances.\n",
    "        else:\n",
    "            appearances += table1.get(key)\n",
    "\n",
    "    #iterate through all of table2's keys\n",
    "    for key,value in table2.items():\n",
    "        #that aren't in table1 \n",
    "        if key not in table1.keys():\n",
    "            #add those to appearances as well.\n",
    "            appearances += table2.get(key)\n",
    "            \n",
    "            \n",
    "    #Return a frequency comparison score equal to the mutual appearances divided by the total appearances.\n",
    "    return round(mutual_appearances/appearances, 2)\n",
    "\n",
    "\n",
    "print (\"% Simiilarity: Lily Trebuchet\")\n",
    "print (frequency_comparison(murderer_sample.word_count_frequency,lily_sample.word_count_frequency))\n",
    "print (\" \")\n",
    "\n",
    "print (\"% Simiilarity: Myrtle Beech\")\n",
    "print (frequency_comparison(murderer_sample.word_count_frequency,myrtle_sample.word_count_frequency))\n",
    "print (\" \")\n",
    "\n",
    "print (\"% Simiilarity: Gregg Fishy\")\n",
    "print (frequency_comparison(murderer_sample.word_count_frequency,gregg_sample.word_count_frequency))\n",
    "print (\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an additional step I took.\n",
    "# I want to generate a list of the mutual ngrams shared by the murderer and each of the contestants.\n",
    "# I want to see what the mutual ngrams were before giving them more consideration in my analysis.\n",
    "# you can see the result in the results area.\n",
    "\n",
    "def frequent_ngram_list(table1, table2):\n",
    "    mutual_ngrams = []\n",
    "\n",
    "    for key,value in table1.items():\n",
    "        if key in table2.keys():\n",
    "            mutual_ngrams.append(key)           \n",
    "    return mutual_ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Average Sentence Length\n",
    "\n",
    "In order to calculate the change between the average sentence lengths of two `TextSamples` we're going to use the formula for the percent difference.\n",
    "\n",
    "Write a function called `percent_difference` that returns the percent difference as calculated from the following formula:\n",
    "\n",
    "$$\\frac{|\\ value1 - value2\\ |}{\\frac{value1 + value2}{2}}$$\n",
    "\n",
    "In the numerator is the absolute value (use `abs()`) of the two values subtracted from each other. In the denominator is the average of the two values (value1 + value2 divided by two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_difference(value1, value2):\n",
    "    return round(abs(value1 - value2)/((value1 + value2)/2),2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scoring Similarity with All Three Indicators\n",
    "\n",
    "We want to figure out who did it, so let's use all three of the indicators we built to score text similarity. Define a function `find_text_similarity` that takes two `TextSample` arguments and returns a float between 0 and 1 where 0 means completely different and 1 means the same exact sample. You can evaluate the similarity by the following criteria:\n",
    "\n",
    "- Calculate the percent difference of their average sentence length using `percent_difference`. Save that into a variable called `sentence_length_difference`. Since we want to find how _similar_ the two passages are calculate the inverse of `sentence_length_difference` by using the formula `abs(1 - sentence_length_difference)`. Save that into a variable called `sentence_length_similarity`.\n",
    "- Calculate the difference between their word usage using `frequency_comparison` on both `TextSample`'s `word_count_frequency` attributes. Save that into a variable called `word_count_similarity`.\n",
    "- Calculate the difference between their two-word ngram using `frequency_table` on both `TextSample`'s `ngram_frequency` attributes. Save that into a variable called `ngram_similarity`.\n",
    "- Add all three similarities together and divide by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text_similarity(textsample1, textsample2):\n",
    "    \n",
    "    \n",
    "    sentence_length_difference = percent_difference(textsample1.average_sentence_length, textsample2.average_sentence_length)\n",
    "    sentence_length_similarity = round(abs(1-sentence_length_difference),2)\n",
    "    \n",
    "    word_count_similarity = frequency_comparison(textsample1.word_count_frequency,textsample2.word_count_frequency)\n",
    "    \n",
    "    ngram_similarity = frequency_comparison(textsample1.ngram_frequency,textsample2.ngram_frequency)\n",
    "    \n",
    "    average_similarity = round((sentence_length_similarity + word_count_similarity + ngram_similarity)/3, 2)\n",
    "    returnString = \"{sls}\\t\\t\\t{wcs}\\t\\t\\t{ngs}\\t\\t{avs}.\".format(author=textsample2.author, sls=sentence_length_similarity, wcs=word_count_similarity, ngs=ngram_similarity, avs=average_similarity)\n",
    "    return returnString\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering the Results\n",
    "\n",
    "We want to print out the results in a way that we can read! For each contestant on _A Brand New Jay_ print out the following:\n",
    "\n",
    "- Their name\n",
    "- Their similarity score to the murder letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "THE RESULTS:\n",
      "\n",
      "NAME\t\tSENTENCE-LENGTH\t\tWORD-COUNT\t\tNGRAM\t\tAVERAGE\n",
      "Murder \t\t 1.0\t\t\t1.0\t\t\t1.0\t\t1.0.\n",
      "Trebuchet \t 0.66\t\t\t0.23\t\t\t0.06\t\t0.32.\n",
      "Fishy \t\t 0.96\t\t\t0.16\t\t\t0.02\t\t0.38.\n",
      "Beech \t\t 0.05\t\t\t0.17\t\t\t0.01\t\t0.08.\n",
      "\n",
      "Common Ngrams:\n",
      "\n",
      "Lily Trebuchet\n",
      "['of us', 'you know', 'all of', 'what he', 'is a', 'for the', 'the cameras', 'do you', 'you remember', 'the first', 'first to', 'to go', 'of them', 'them very', 'very much', 'much and', 'with him', 'one of', \"it's definitely\", 'definitely the', 'you can', 'can feel', 'feel in', 'in your', 'he is', 'to be', 'every bit', 'bit of', 'know jay', 'jay stacksby', 'stacksby is', 'of all', 'and the']\n",
      " \n",
      "Gregg Fishy\n",
      "['that jay', 'like a', 'for what', 'one of', 'on the', 'i am', 'to be', 'and i', 'that he']\n",
      " \n",
      "Myrtle Beech\n",
      "['to the', 'for the', 'to go', 'he called', 'i am', 'go on']\n",
      " \n",
      "\n",
      "Favorite Words:\n",
      "\n",
      "Lily Trebuchet\n",
      "['he', 'the', 'a', 'to', 'was', 'and', 'all', 'him', 'of', 'that', 'i', 'you', 'like', 'us', 'know', 'things', 'with', 'live', 'in', 'what', 'is', 'but', 'jay', 'this', 'for', 'go', 'up', 'who', 'me', \"i'm\", 'whole', 'think', 'cameras', 'off', 'do', 'remember', 'first', 'my', 'them', 'very', 'much', 'one', \"it's\", 'definitely', 'can', 'feel', 'your', 'how', 'well', 'be', 'some', 'man', 'every', 'bit', 'stacksby', 'without']\n",
      " \n",
      "Gregg Fishy\n",
      "['he', 'the', 'a', 'to', 'and', 'all', 'of', 'that', 'i', 'you', 'like', 'with', 'in', 'what', 'jay', 'for', 'never', 'on', 'who', 'going', \"i'm\", 'an', 'do', 'remember', 'my', 'one', 'am', 'be', 'some', 'when', 'every']\n",
      " \n",
      "Myrtle Beech\n",
      "['he', 'the', 'a', 'to', 'was', 'and', 'of', 'that', 'i', 'you', 'like', 'things', 'with', 'live', 'is', 'jay', 'this', 'for', 'go', 'up', 'we', 'on', 'would', 'me', 'money', 'called', 'such', 'my', 'had', 'did', 'one', 'last', 'can', 'am', 'be', 'when', 'man', 'every', 'better', 'without']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print (\"\\n\")\n",
    "print (\"THE RESULTS:\\n\")\n",
    "\n",
    "print (\"NAME\\t\\tSENTENCE-LENGTH\\t\\tWORD-COUNT\\t\\tNGRAM\\t\\tAVERAGE\")\n",
    "print (murderer_sample.author,\"\\t\\t\", find_text_similarity(murderer_sample, murderer_sample))\n",
    "print (lily_sample.author,\"\\t\", find_text_similarity(murderer_sample, lily_sample))\n",
    "print (gregg_sample.author,\"\\t\\t\", find_text_similarity(murderer_sample, gregg_sample))\n",
    "print (myrtle_sample.author,\"\\t\\t\", find_text_similarity(murderer_sample, myrtle_sample))\n",
    "\n",
    "print (\"\")\n",
    "\n",
    "print (\"Common Ngrams:\\n\")\n",
    "print (\"Lily Trebuchet\")\n",
    "print (frequent_ngram_list(murderer_sample.ngram_frequency,lily_sample.ngram_frequency))\n",
    "print (\" \")\n",
    "\n",
    "print (\"Gregg Fishy\")\n",
    "print (frequent_ngram_list(murderer_sample.ngram_frequency,gregg_sample.ngram_frequency))\n",
    "print (\" \")\n",
    "\n",
    "print (\"Myrtle Beech\")\n",
    "print (frequent_ngram_list(murderer_sample.ngram_frequency,myrtle_sample.ngram_frequency))\n",
    "print (\" \")\n",
    "\n",
    "print (\"\")\n",
    "print (\"Favorite Words:\\n\")\n",
    "print (\"Lily Trebuchet\")\n",
    "print (frequent_ngram_list(murderer_sample.word_count_frequency,lily_sample.word_count_frequency))\n",
    "print (\" \")\n",
    "\n",
    "print (\"Gregg Fishy\")\n",
    "print (frequent_ngram_list(murderer_sample.word_count_frequency,gregg_sample.word_count_frequency))\n",
    "print (\" \")\n",
    "\n",
    "print (\"Myrtle Beech\")\n",
    "print (frequent_ngram_list(murderer_sample.word_count_frequency,myrtle_sample.word_count_frequency))\n",
    "print (\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problems with looking at the average of the three scores. It is easy to look at the average scores and determine that Fishy has the highest Average Similarity. There are some issues with this. His score of .38 is only slightly higher than Trebuchet's Average Score of .32.\n",
    "\n",
    "Fishy's high average score is a result of his really high sentence length score. As a group the sentence length scores are big numbers relative to the other scores. Taking the average gives more weight to these big numbers. The average favors setence length. While Ngram Similarity & Word Count Similarity are relativly smaller values, the measurement is more informative than sentence length. \n",
    "\n",
    "Trebuchet actually scored higher in two of the three categories: Ngram Similarity, & Word Count Similarity. Significantly higher. Her common bigrams triple Fishy's. She She has almost 1.5 times as many common favorite words as he does. Trebuchet has a sentence length similarity of 66%. That's pretty high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who Dunnit?\n",
    "\n",
    "In the cell below, print the name of the person who killed Jay Stacksby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lily Trebuchet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
